TODO: MLM and CLM need to tie weights to be fully equivalent to HuggingFace.

TODO: SuperGLUE

TODO: Dataset caching. For hyperparameter tuning, you don't want to keep calling .map() each time.

TODO: MNLI validation and test set should be switched (now test set comes from train and that's easier)

TODO: When device batch is set bigger than the effective batches, it has to be lowered.

TODO: The interval/stopping hyperparameter classes that require a batch size are redundant and can go out of sync.
      E.g., if you change the batch size in given hyperparameters, you don't know if you should change the intervals too.
      Bad design. Should be able to refer to the batch size inside the hyperparameter object.

TODO: I would like more decoupling between evaluation and saving.
    - Be able to save more frequently than evals so that you don't lose hours of progress just because you're scared of the cost of eval.
    - Be able to delete models after training, which is what you need for phase 1 of finetuning.

TODO: Hour-long backoff wrapper around streamed datasets, to smooth over HuggingFace outages when streaming.
    - Also spice it up by sending the user an email to notify them of this "hybernation mode".
    - Also save the model.

TODO: It would be very cool if there was a callback connected to some kind of messaging
      service, where you could send a signal to kill training and the model will (after the current step) quit and save.

TODO: Test the resume-from-folder functionality. It isn't properly tested.

TODO: A framework for hyperparameter tuning as follows (I have this implemented in WiaT):
    - Given are ranges for various hyperparameters, an amount of samples, and a maximal amount of descents.
    - for _ in range(samples): run fine-tuning for a max amount of descents. Could be many epochs, could be less than 1 epoch. Evaluate on the full validation set.
    - Select the sampled set of hyperparameters with the best validation set performance.
    - Re-run finetuning with that set WITHOUT max amount of descents, with early stopping on the validation set.
    - Evaluate the result on the test set.

TODO: Tokeniser parallelisation.

TODO: in the Task.train core,
    - Should the optimisers be given as training parameters to allow the use of accelerate (and perhaps multi-GPU)?
    - I wonder if .train(resume_from_checkpoint) keeps instance-level architecture or acts like .from_pretrained() in that it resets the architecture.

TODO: Implement QA. I wonder if you need a logit transform for this too. The Trainer tutorial
       for QA sidesteps this question: https://huggingface.co/docs/transformers/tasks/question_answering#evaluate

TODO: I'm not happy with how metrics are set up. It really makes zero sense to embed constructor arguments inside
      the task config to then parse them implicitly (if you didn't forget them) or error (because you did forget them).
